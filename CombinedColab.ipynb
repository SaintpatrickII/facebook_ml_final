{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CombinedColab.ipynb",
      "provenance": [],
      "mount_file_id": "1qwgQDX88jjegtGoN_kZRIDsmNhNV2RMH",
      "authorship_tag": "ABX9TyNDu/AXqBxX3XiFFntPFl/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaintpatrickII/facebook_ml/blob/main/CombinedColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-IbgGiJQR9O"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "from tqdm import tqdm\n",
        "from combined_dataloader import ImageTextDataloader\n",
        "import torch\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import Tensor\n",
        "import torch.nn as nn \n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "# from pytorch_loader import ImagesLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import io\n",
        "import torchvision.transforms as transforms\n",
        "import pickle\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/Images.zip'\n",
        "!cp “{zip_path}” .\n",
        "!unzip -q Images.zip\n",
        "!rm Images.zip\n",
        "\n",
        "products_df = '/Users/paddy/Desktop/AiCore/facebook_ml/final_dataset/combined_final_dataset.csv'\n",
        "image_folder = '/Users/paddy/Desktop/AiCore/facebook_ml/images_for_combined/'\n",
        "\n",
        "\n",
        "\n",
        "from skimage import io\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "\n",
        "products_df = '/Users/paddy/Desktop/AiCore/facebook_ml/final_dataset/combined_final_dataset.csv'\n",
        "image_folder = '/Users/paddy/Desktop/AiCore/facebook_ml/images_for_combined/'\n",
        "batch_size = 32\n",
        "\n",
        "class ImageTextDataloader(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, transform: transforms = None, labels_level : int=0, max_desc_len = 50):\n",
        "        self.products = pd.read_csv(products_df, lineterminator='\\n')\n",
        "        self.root_dir = image_folder\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "        self.max_desc_len = max_desc_len\n",
        "        self.products['category'] = self.products['category'].apply(lambda x: self.get_category(x, labels_level))\n",
        "        self.descriptions = self.products['product_description']\n",
        "        self.image_id = self.products['image_id']\n",
        "        self.labels = self.products['category'].to_list()\n",
        "        self.num_classes = len(set(self.labels))\n",
        "\n",
        "\n",
        "        self.encoder = {y: x for (x, y) in enumerate(set(self.labels))}\n",
        "        self.decoder = {x: y for (x, y) in enumerate(set(self.labels))}\n",
        "\n",
        "        if transform == None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.3),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.CenterCrop(128),\n",
        "                transforms.Resize(128),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225])\n",
        "                ])\n",
        "\n",
        "\n",
        "        self.tokenizer = get_tokenizer('basic_english')\n",
        "        self.vocab = self.get_vocab()\n",
        "        # self.descriptions = self.tokenize_descriptions(self.descriptions)\n",
        "\n",
        "        assert len(self.descriptions) == len(self.labels) == len(self.image_id)\n",
        "    \n",
        "\n",
        "    def get_vocab(self):\n",
        "\n",
        "        def yield_tokens():\n",
        "            for description in self.descriptions:\n",
        "                tokens = self.tokenizer(description)\n",
        "                yield tokens\n",
        "        token_generator = yield_tokens()\n",
        "\n",
        "        vocab = build_vocab_from_iterator(token_generator, specials=['<UNK>'])\n",
        "        print('length of vocab:', len(vocab))\n",
        "        return vocab\n",
        "\n",
        "\n",
        "    def tokenize_descriptions(self, descriptions):\n",
        "        def tokenize_description(description):\n",
        "            words = self.tokenizer(description)\n",
        "            words = words[:50]\n",
        "            pad_length = self.max_desc_len - len(words)\n",
        "            words.extend(['<UNK>'] * pad_length)\n",
        "            tokenized_desc = self.vocab(words)\n",
        "            tokenized_desc = torch.tensor(tokenized_desc)\n",
        "            return tokenized_desc\n",
        "\n",
        "        descriptions = tokenize_description(descriptions)\n",
        "        # .apply(tokenize_description)\n",
        "        return descriptions\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.products)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        label = self.encoder[label]\n",
        "        label = torch.as_tensor(label)\n",
        "        image = Image.open(self.root_dir + (self.products.iloc[index, 1] + '.jpg')).convert('RGB')\n",
        "        # print(image)\n",
        "        image = self.transform(image)\n",
        "        # print('this is image', image)\n",
        "        sentence = self.descriptions[index]\n",
        "        # print(sentence)\n",
        "        encoded = self.tokenize_descriptions(sentence)\n",
        "        # print(encoded)\n",
        "        description = encoded\n",
        "        # encoded = self.tokenize_descriptions(sentence)\n",
        "        # encoded = {key:torch.LongTensor(value) for key, value in encoded.items()}\n",
        "        # with torch.no_grad():\n",
        "        #     description = self.model(**encoded).last_hidden_state.swapaxes(1,2)\n",
        "\n",
        "        # description = description.squeeze(0)\n",
        "        # return (image, label)\n",
        "        return image, description, label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Picking GPU if available or else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "device = get_default_device()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TextClassifier(torch.nn.Module):\n",
        "    def __init__(self, pretrained_weights=None):\n",
        "        super().__init__()\n",
        "        no_words = 28381\n",
        "        embedding_size = 50\n",
        "        self.embedding = torch.nn.Embedding(no_words, embedding_size)\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(embedding_size, 32, 2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv1d(32, 64, 2),\n",
        "            torch.nn.Dropout(),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(3072, 128)\n",
        "        )\n",
        "\n",
        "\n",
        "    '''\n",
        "    TextClassifier Initiliser:\n",
        "\n",
        "    We have an embedding layer, which is a matrix of size 26888x100, which is the size of our\n",
        "    vocabulary. from here a TextClassifier is built using dropout & ReLU to avoid overfitting\n",
        "    '''\n",
        "\n",
        "    def forward(self, X):\n",
        "        \n",
        "        return self.layers(self.embedding(X))\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    forward: \n",
        "\n",
        "    The function takes in a batch of sentences, passes them through the embedding layer, and then\n",
        "    passes them through the layers of the model\n",
        "    :param X: the input data\n",
        "    :return: The output of the last layer of the network.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "class ImageTextClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageTextClassifier, self).__init__()\n",
        "        self.features = models.resnet50(pretrained=True).to(device)\n",
        "        self.text_model = TextClassifier()\n",
        "        self.main = nn.Sequential(nn.Linear(256, 13))\n",
        "        for i, param in enumerate(self.features.parameters()):\n",
        "            if i < 47:\n",
        "                param.requires_grad=False\n",
        "            else:\n",
        "                param.requires_grad=True\n",
        "        self.features.fc = nn.Sequential(\n",
        "            nn.Linear(2048, 1024), # first arg is the size of the flattened output from resnet50\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(),\n",
        "            torch.nn.Linear(1024, 512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, 128)\n",
        "            # torch.nn.ReLU(),\n",
        "            # torch.nn.Linear((128), 13)\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, image_features, text_features):\n",
        "        image_features = self.features(image_features)\n",
        "        image_features = image_features.reshape(image_features.shape[0], -1)\n",
        "        text_features = self.text_model(text_features)\n",
        "        combined_features = torch.cat((image_features, text_features), 1)\n",
        "\n",
        "\n",
        "\n",
        "        # x = torch.nn.Softmax(dim=1)(x)\n",
        "        return combined_features\n",
        "\n",
        " \n",
        "model = ImageTextClassifier()\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "dataset = ImageTextDataloader()\n",
        "dataloader = dataloader = torch.utils.data.DataLoader(dataset, batch_size=32 ,shuffle=True, num_workers=1)\n",
        "# print(dataset[5000])\n",
        "\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs\n",
        "\n",
        "\n",
        "def train_model(model, epochs):\n",
        "# optimiser):\n",
        "# scheduler\n",
        "    writer = SummaryWriter()\n",
        "    print('training model')\n",
        "    # dataset_ite = tqdm(enumerate(dataloader))\n",
        "    optimiser = optim.SGD(model.parameters(), lr=0.01)\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch + 1}/{epochs}')\n",
        "        print('-' * 10)\n",
        "        for i, (image_features, text_features, labels) in tqdm(enumerate(dataloader)):\n",
        "            model.train()\n",
        "            num_correct = 0\n",
        "            num_samples = 0\n",
        "            image_features = image_features.to(device)\n",
        "            text_features = text_features.to(device)  # move to device\n",
        "            labels = labels.to(device)\n",
        "            predict = model(image_features, text_features)\n",
        "            labels = labels\n",
        "\n",
        "\n",
        "            loss = F.cross_entropy(predict, labels)\n",
        "            _, preds = predict.max(1)\n",
        "            num_correct += (preds == labels).sum()\n",
        "            num_samples += preds.size(0)\n",
        "            acc = float(num_correct) / num_samples\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "\n",
        "            if i % 10 == 9:\n",
        "                writer.add_scalar('Training Loss', loss, epoch)\n",
        "                writer.add_scalar(' Training Accuracy', acc, epoch)\n",
        "                print('training_loss')\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {loss:.5f}')\n",
        "                print(f'Got {num_correct} / {num_samples} with accuracy: {(acc * 100):.2f}%')\n",
        "                writer.flush()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    print('Checking accuracy on training set')\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for (image_features, text_features, label) in tqdm(loader):\n",
        "            image_features = image_features.to(device)\n",
        "            text_features = text_features.to(device)  # move to device\n",
        "            label = label.to(device)\n",
        "            predict = model(image_features, text_features)\n",
        "            _, preds = predict.max(1)\n",
        "            num_correct += (preds == label).sum()\n",
        "            num_samples += preds.size(0)\n",
        "        acc = float(num_correct) / num_samples\n",
        "        print(f'Got {num_correct} / {num_samples} with accuracy: {acc * 100}%')\n",
        "        \n",
        "\n",
        "    model_save_name = 'combined.pt'\n",
        "    path = f\"/Users/paddy/Desktop/AiCore/facebook_ml/{model_save_name}\" \n",
        "    torch.save(model.state_dict(), path)\n",
        "    with open('combined_decoder.pkl', 'wb') as f:\n",
        "        pickle.dump(dataset.decoder, f)\n",
        "                    \n",
        "if __name__ == '__main__':\n",
        "    train_model(model, 50)\n",
        "    check_accuracy(dataloader, model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SpjrUkWFQ68u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}